{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100 - Final project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boostibot/BasicNeuralNet/blob/main/100_Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ZZfC2Rl9f4"
      },
      "source": [
        "# The Goal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKy9U3s_rzSv"
      },
      "source": [
        "Determine if a person has diabetes based on variety of mostly binary health indicators.\n",
        "\n",
        "Additionally, find which of the indicators are the most valuable for the purposes of classification, seek to understand the decision making process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNAqrM59Sp0S"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XwYQA7ag9m"
      },
      "source": [
        "Split the code blocks as you need them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlPp5EXLirb6"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "**[Diabetes health indicators](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset):**\n",
        "\n",
        "The dataset is comprised of 21 mostly binary health indicators and a class healthy or diabetes. The dataset is balanced and contains 50/50 split between patients with and without diabetes. The health indicators vary greatly in distribution and importance. For example the features high blood pressure/no, age bracket, high cholesterol/no, general health indicator are highly influencial and can be readily used. On the other hand features such as eats veggies/no, has insurrance/no or even absurdly had cholesterol check in the last 5 years are all either very skewed in distribution or simply dont correspond with diabetes whatsoever.\n",
        "\n",
        "Below are shown the distributions of each of the features. Additionally each bar is split into healty (blue) and diabetes (orange).\n",
        "\n",
        "<img src=\"https://staff.utia.cas.cz/novozada/ml1/dummy.png\">\n",
        "\n",
        "To better determine which features might be useful (or if any!) I also plot the relative difference between the two classes. That is I plot the histogram $h_\\text{diff}(i) = (h_d(i) - h_h(i))/h(i)$ where $i$ is the $i$-th unqiue value of the feature (histogram x value) and $h(i)$ its count in the histogram (y value). $h_d$ and $h_h$ are histograms of just the people with diabetes and without respectively.\n",
        "\n",
        "<img src=\"https://staff.utia.cas.cz/novozada/ml1/dummy.png\">\n",
        "\n",
        "This largely verifies what I stated in the overview of the dataset. Nearly all features behave as one would expect, perhaps with the exception of heavy alcohol use which shows that people who often consume alcohol have significantly lower rates of diabetes than those who do. This is, however, probably caused by outliers as there is very small number of such individuals in this dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sG6-apMCtlC"
      },
      "source": [
        "import numpy as np\n",
        "def load_csv_dataset(path: str, delimiter: str = \",\") -> tuple:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        header = f.readline().rstrip(\"\\n\\r\")\n",
        "    labels = np.array([col.strip() for col in header.split(delimiter)], dtype=str)\n",
        "    data = np.loadtxt(path, delimiter=delimiter, skiprows=1)\n",
        "    if data.ndim == 1:  # single data row -> ensure 2D shape (1, n_cols)\n",
        "        data = data.reshape(1, -1)\n",
        "    return data, labels\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset, labels = load_csv_dataset(\"diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n",
        "\n",
        "#Train test split\n",
        "X, y = dataset[:,1:], dataset[:,0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owqzIA6Mi2wA"
      },
      "source": [
        "### Decision trees, random forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMv6Fk0zX-f"
      },
      "source": [
        "I chose decision trees and random forests because they are highly interpretable. One of the goals of this project was to better understand the underlying data, which features are useful and which are not and decision making process as a whole.\n",
        "\n",
        "Second, I wanted to experiment with random forests to see if they improve the accuracy on this noisy dataset.\n",
        "\n",
        "For both models I used the implementation provided by scikit-learn. The used formulation corresponds directly to the one introduced in these labs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qI9C2PhjDJP"
      },
      "source": [
        "class MyDummyClassifier:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    self._classes = np.unique(y)\n",
        "    self._priors = [X[c==y].shape[0] / float(n_samples) for c in self._classes]\n",
        "\n",
        "  def predict(self, X):\n",
        "    print(self._priors)\n",
        "    return np.random.choice(self._classes, X.shape[0], p=self._priors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL-3MkCTjGbW"
      },
      "source": [
        "### Train your classifier(s) and predict the testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxaVVzZCjTQ5",
        "outputId": "6ee7061c-862e-4a90-ba40-8dbb3df2dd6c"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "  return accuracy\n",
        "\n",
        "# Design a model\n",
        "clf = MyDummyClassifier()\n",
        "\n",
        "# 4. Train a model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# 6. Evaluate predictions\n",
        "print(\"Dummy classifier has accuracy\", accuracy(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.12625, 0.87375]\n",
            "Dummy classifier has accuracy 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFA8079oj8xY"
      },
      "source": [
        "## Discuss the results\n",
        "\n",
        "We find that surprisingly even very small trees predict the data very well. We also find that large trees are prone to overfitting and achive worse results than even the smallest tree tested.\n",
        "\n",
        "Random forests ever-so-slightly improve on the test results but are probably not worth doing for their additional cost and train time for this dataset.\n",
        "\n",
        "What do the trees look like? Heres a tree with 20 leaf nodes visualized with grapviz.\n",
        "\n",
        "...\n",
        "\n",
        "as you can see some of the nodes are redunant and the tree can be simplified. For this we write this rather complicated recursive function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collapse_same_class_subtrees(clf):\n",
        "    \"\"\"\n",
        "    Modify clf.tree_ in place to collapse any node whose entire subtree\n",
        "    predicts a single class into a leaf node.\n",
        "    \"\"\"\n",
        "    from sklearn.tree import _tree\n",
        "    tree = clf.tree_\n",
        "    children_left = tree.children_left\n",
        "    children_right = tree.children_right\n",
        "    value = tree.value\n",
        "\n",
        "    def recurse(node):\n",
        "        # if node is leaf => return predicted class index\n",
        "        if children_left[node] == _tree.TREE_LEAF and children_right[node] == _tree.TREE_LEAF:\n",
        "            return np.argmax(value[node])\n",
        "\n",
        "        # otherwise check children\n",
        "        left_child = children_left[node]\n",
        "        right_child = children_right[node]\n",
        "        left_class = recurse(left_child)\n",
        "        right_class = recurse(right_child)\n",
        "\n",
        "        # if both children subtrees exist and both predict same single class\n",
        "        if (left_class is not None) and (right_class is not None) and (left_class == right_class):\n",
        "            # collapse node -> make it a leaf\n",
        "            tree.children_left[node] = _tree.TREE_LEAF\n",
        "            tree.children_right[node] = _tree.TREE_LEAF\n",
        "            tree.feature[node] = _tree.TREE_UNDEFINED\n",
        "            tree.threshold[node] = -2.0\n",
        "            # aggregate counts/values from children (their value equals subtree counts)\n",
        "            tree.value[node] = tree.value[left_child] + tree.value[right_child]\n",
        "            return left_class\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    recurse(0)  # root"
      ],
      "metadata": {
        "id": "cCN7CTz0IqII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying this function and regenerating the graph shows us this rather simple decision tree.\n",
        "\n",
        "...\n",
        "\n",
        "Lastly, we can see that most paths through the tree terminate rather quickly in one of the leafs, yet there are some longer paths possible as well. Indeed, if we look at the counts we can see that just the top few nodes capture most of the instances. So what would it look like if we simplified further?\n",
        "\n",
        "Here is (simplified) tree of just two levels. The classification using this tree is extemely simple, yet even such a tiny tree gives better results than the overly large tree we started with!\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "ll6B7iCuIrdy"
      }
    }
  ]
}